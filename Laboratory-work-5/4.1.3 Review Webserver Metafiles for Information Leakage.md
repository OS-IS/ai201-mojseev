# WSTG - Stable
## Home > Stable > 4-Web Application Security Testing > 01-Information Gathering

## Огляд метафайлів вебсервера на витік інформації
**ID**: WSTG-INFO-03

## Резюме
Цей розділ описує, як тестувати різні метадані файли на витік інформації про шляхи вебзастосунку або функціональність. Крім того, можна створити список директорій, яких варто уникати для пошукових роботів або краулінгів, що є залежністю для мапування шляхів через застосунок. Також може бути зібрана інша інформація для визначення поверхні атаки, технічних подробиць або для використання у соціальному інженірингу.

## Мета тесту
- Визначити приховані або зашифровані шляхи та функціональність через аналіз метаданих файлів.
- Витягти й відобразити іншу інформацію, що може допомогти краще зрозуміти системи.

## Як тестувати
Будь-які з дій, виконаних нижче за допомогою `wget`, можуть також бути виконані за допомогою `curl`. Багато інструментів для динамічного тестування безпеки вебзастосунків (DAST), таких як ZAP та Burp Suite, включають перевірки або парсинг цих ресурсів як частину їх функціональності для краулінгу/спайдерства. Їх також можна ідентифікувати за допомогою різних Google Dorks або використовуючи розширені пошукові можливості, такі як `inurl:`.

## Robots
Веб-спайдери, роботи або краулери отримують вебсторінку, а потім рекурсивно переходять за гіперпосиланнями для отримання додаткового контенту. Їх поведінка визначається за допомогою протоколу виключення роботів у файлі `robots.txt`, який розташований у кореневому каталозі вебсервера.

Приклад початку файлу `robots.txt` від Google, отриманий 5 травня 2020 року:
| **Directive**      | **Path**                |
|--------------------|-------------------------|
| User-agent         | *                       |
| Disallow           | /search                 |
| Allow              | /search/about           |
| Allow              | /search/static          |
| Allow              | /search/howsearchworks  |
| Disallow           | /sdch                   |

Директива User-Agent относится к определенному веб-пауку/роботу/краулеру. Например, User-Agent: Googlebot относится к пауку от Google, а User-Agent: bingbot относится к краулеру от Microsoft. User-Agent: * в примере выше применяется ко всем веб-паукам/роботам/краулерам.

Директива Disallow указывает, какие ресурсы запрещены пауками/роботами/краулерами. В примере выше запрещены следующие:
| **Directive**      | **Path**        |
|--------------------|-----------------|
| Disallow           | /search         |
| Disallow           | /sdch           |

Веб-пауки/роботы/краулеры могут намеренно игнорировать директивы Disallow, указанные в файле robots.txt, например, из социальных сетей, чтобы гарантировать, что общие ссылки по-прежнему действительны. Следовательно, robots.txt не следует рассматривать как механизм для навязывания ограничений на то, как веб-контент доступен, хранится или переиздается третьими лицами.

Файл robots.txt извлекается из корневого веб-каталога веб-сервера. Например, чтобы извлечь robots.txt из www.google.com с помощью wget или curl:

| **Command**                                                                            | **Output**                                                                                     |
|----------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------|
| `$ curl -O -Ss http://www.google.com/robots.txt && head -n5 robots.txt`                | User-agent: *                                                                                 |
|                                                                                        | Disallow: /search                                                                              |
|                                                                                        | Allow: /search/about                                                                          |
|                                                                                        | Allow: /search/static                                                                         |
|                                                                                        | Allow: /search/howsearchworks                                                                 |

## Аналіз robots.txt за допомогою Google Webmaster Tools

Власники вебсайтів можуть використовувати функцію Google "Аналіз robots.txt" для аналізу вебсайту в рамках Google Webmaster Tools. Цей інструмент може допомогти з тестуванням, і процедура виглядає наступним чином:

1. Увійти в Google Webmaster Tools за допомогою облікового запису Google.
2. На панелі інструментів введіть URL для сайту, який потрібно проаналізувати.
3. Виберіть один із доступних методів і дотримуйтесь інструкцій на екрані.

## META теги

Теги `<META>` знаходяться в секції HEAD кожного HTML-документу і повинні бути однаковими на всьому вебсайті, якщо точка початку для робота/спайдера/краулера не є документом, що посилається на глибоке посилання (наприклад, не на головну сторінку). Директиву роботів також можна вказати через використання специфічного META тега.

## Robots META тег

Якщо в документі відсутній запис `<META NAME="ROBOTS" ... >`, то за умовчанням застосовується "Протокол виключення роботів" з параметрами `INDEX,FOLLOW`. Тому інші два допустимі записи, визначені цим протоколом, мають префікс `NO...`, тобто `NOINDEX` і `NOFOLLOW`.

Згідно з директивами `Disallow`, вказаними в файлі robots.txt у вебкореневій директорії, проводиться пошук за допомогою регулярних виразів для `<META NAME="ROBOTS"` на кожній веб-сторінці і результати порівнюються з файлом robots.txt.

## Різні META інформаційні теги

Організації часто вбудовують інформаційні META теги в вебконтент для підтримки різних технологій, таких як екранні зчитувачі, попередні перегляди в соціальних мережах, індексація пошуковими системами тощо. Така мета-інформація може бути корисною для тестувальників при визначенні використовуваних технологій та додаткових шляхів/функціональностей для дослідження і тестування.

Наступну мета-інформацію було отримано з [www.whitehouse.gov](https://www.whitehouse.gov) через перегляд джерела сторінки 5 травня 2020 року:

```html
<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="The White House" />
<meta property="og:description" content="Ми, громадяни Америки, зараз об'єднані в великій національній зусиллі відновити нашу країну та відновити її обіцянки для всіх. – Президент Дональд Трамп." />
<meta property="og:url" content="https://www.whitehouse.gov/" />
<meta property="og:site_name" content="The White House" />
<meta property="fb:app_id" content="1790466490985150" />
<meta property="og:image" content="https://www.whitehouse.gov/wp-content/uploads/2017/12/wh.gov-share-img_03-1024x538.png" />
<meta property="og:image:secure_url" content="https://www.whitehouse.gov/wp-content/uploads/2017/12/wh.gov-share-img_03-1024x538.png" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:description" content="Ми, громадяни Америки, зараз об'єднані в великій національній зусиллі відновити нашу країну та відновити її обіцянки для всіх. – Президент Дональд Трамп." />
<meta name="twitter:title" content="The White House" />
<meta name="twitter:site" content="@whitehouse" />
<meta name="twitter:image" content="https://www.whitehouse.gov/wp-content/uploads/2017/12/wh.gov-share-img_03-1024x538.png" />
<meta name="twitter:creator" content="@whitehouse" />
<meta name="apple-mobile-web-app-title" content="The White House">
<meta name="application-name" content="The White House">
<meta name="msapplication-TileColor" content="#0c2644">
<meta name="theme-color" content="#f5f5f5">
```

## Картки сайту (Sitemaps)
Картка сайту — це файл, в якому розробник або організація може надавати інформацію про сторінки, відео та інші файли, які пропонуються сайтом або додатком, а також про взаємозв'язок між ними. Пошукові системи можуть використовувати цей файл для більш інтелектуального дослідження вашого сайту. Тестувальники можуть використовувати файли sitemap.xml, щоб дізнатися більше про сайт чи додаток і дослідити його більш повно.

Наступний уривок взятий з основної картки сайту Google, отриманої 5 травня 2020 року.

```
$ wget --no-verbose https://www.google.com/sitemap.xml && head -n8 sitemap.xml
2020-05-05 12:23:30 URL:https://www.google.com/sitemap.xml [2049] -> "sitemap.xml" [1]

<?xml version="1.0" encoding="UTF-8"?>
<sitemapindex xmlns="http://www.google.com/schemas/sitemap/0.84">
  <sitemap>
    <loc>https://www.google.com/gmail/sitemap.xml</loc>
  </sitemap>
  <sitemap>
    <loc>https://www.google.com/forms/sitemaps.xml</loc>
  </sitemap>
...
```
Исследуя этот сайт, тестировщик может захотеть получить карту сайта Gmail https://www.google.com/gmail/sitemap.xml:
```
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9" xmlns:xhtml="http://www.w3.org/1999/xhtml">
  <url>
    <loc>https://www.google.com/intl/am/gmail/about/</loc>
    <xhtml:link href="https://www.google.com/gmail/about/" hreflang="x-default" rel="alternate"/>
    <xhtml:link href="https://www.google.com/intl/el/gmail/about/" hreflang="el" rel="alternate"/>
    <xhtml:link href="https://www.google.com/intl/it/gmail/about/" hreflang="it" rel="alternate"/>
    <xhtml:link href="https://www.google.com/intl/ar/gmail/about/" hreflang="ar" rel="alternate"/>
...
```
## Security TXT

**security.txt** — це запропонований стандарт, який дозволяє вебсайтам визначати свої політики безпеки та контактні дані. Це може бути корисно в сценаріях тестування з кількох причин, зокрема, але не обмежуючись:

- Ідентифікація додаткових шляхів або ресурсів для включення в аналіз.
- Збір відкритої інформації (Open Source Intelligence).
- Пошук інформації про програми Bug Bounty та інше.
- Соціальна інженерія.

Файл може бути розміщений або в кореневій директорії вебсервера, або в директорії `.well-known/`. Приклади:

- `https://example.com/security.txt`
- `https://example.com/.well-known/security.txt`

Ось приклад з реального світу, отриманий з LinkedIn 5 травня 2020 року:
```
$ wget --no-verbose https://www.linkedin.com/.well-known/security.txt && cat security.txt
2020-05-07 12:56:51 URL:https://www.linkedin.com/.well-known/security.txt [333/333] -> "security.txt" [1]
# Conforms to IETF `draft-foudil-securitytxt-07`
Contact: mailto:security@linkedin.com
Contact: https://www.linkedin.com/help/linkedin/answer/62924
Encryption: https://www.linkedin.com/help/linkedin/answer/79676
Canonical: https://www.linkedin.com/.well-known/security.txt
Policy: https://www.linkedin.com/help/linkedin/answer/62924
```
## Humans TXT

**humans.txt** — це ініціатива, спрямована на визначення людей, які стоять за створенням вебсайту. Він має вигляд текстового файлу, який містить інформацію про різних людей, які брали участь у розробці сайту. Детальніше можна дізнатися на сайті [humanstxt](https://humanstxt.org/). Цей файл часто (хоча і не завжди) містить інформацію для кар'єрних сайтів або шляхів працевлаштування.

Ось приклад з реального світу, отриманий з Google 5 травня 2020 року:
```
$ wget --no-verbose  https://www.google.com/humans.txt && cat humans.txt
2020-05-07 12:57:52 URL:https://www.google.com/humans.txt [286/286] -> "humans.txt" [1]
Google is built by a large team of engineers, designers, researchers, robots, and others in many different sites across the globe. It is updated continuously, and built with more tools and technologies than we can shake a stick at. If you'd like to help us out, see careers.google.com.
```
## Інші джерела інформації в .well-known

Існують інші RFC та Інтернет-драфти, які пропонують стандартизовані способи використання файлів у директорії `.well-known/`. Перелік таких файлів можна знайти [тут](https://www.ietf.org) або [тут](https://datatracker.ietf.org/).

Тестувальнику буде досить просто переглянути RFC/драфти та створити список, який можна передати краулеру або фузеру для перевірки наявності чи вмісту таких файлів.

## Інструменти
- Браузер (перегляд джерела сторінки або функціональність інструментів розробника)
- curl
- wget
- Burp Suite
- ZAP

